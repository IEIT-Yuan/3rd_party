


<div align="center">
<h1>
  llama.cpp for Yuan2.0å¤§æ¨¡å‹
</h1>
</div>


<p align="center">
ğŸ“£ <a href="https://github.com/IEIT-Yuan/Yuan2.0-M32" target="_blank">æº2.0 M32å¤§æ¨¡å‹</a> â€¢ğŸ“ <a href="https://github.com/IEIT-Yuan/Yuan-2.0" target="_blank">æº2.0 å¤§æ¨¡å‹</a> â€¢  ğŸ’¬ <a href="https://github.com/IEIT-Yuan/YuanChat" target="_blank">YuanChat</a>â€¢ ğŸ“  <a href="https://arxiv.org/abs/2405.17976" target="_blank">æº2.0 M32è®ºæ–‡</a>
</p>



<div align="center">

    
  <a href="code_license">
    <img alt="Code License" src="https://img.shields.io/badge/Apache%202.0%20-green?style=flat&label=Code%20License&link=https%3A%2F%2Fgithub.com%2FIEIT-Yuan%2FYuan-2.0-MoE%3Ftab%3DApache-2.0-1-ov-file"/>
  </a>
  <a href="model_license">
    <img alt="Model License" src="https://img.shields.io/badge/Yuan2.0%20License-blue?style=flat&logoColor=blue&label=Model%20License&color=blue&link=https%3A%2F%2Fgithub.com%2FIEIT-Yuan%2FYuan-2.0%2Fblob%2Fmain%2FLICENSE-Yuan" />
  </a>

</div>

## 0. Introduction

æˆ‘ä»¬ä¸ºå¼€å‘è€…æä¾›äº†é€‚ç”¨äºYuan2.0å¤§æ¨¡å‹ çš„ llama.cppï¼ˆllama.cpp for Yuan2.0ï¼‰é¡¹ç›®ï¼ŒåŸºäºå¼€æºé¡¹ç›®[llama.cpp](https://github.com/ggerganov/llama.cpp)ï¼Œæä¾›æ¨¡å‹è½¬æ¢ã€é‡åŒ–åŠŸèƒ½çš„å®Œæ•´ä»£ç ä¸è¯¦ç»†æ­¥éª¤ï¼Œæ”¯æŒä½¿ç”¨Yuan2.0-M32ä¸Yuan2.0å¤§æ¨¡å‹æœ¬åœ°ã€æœåŠ¡æœåŠ¡å™¨ç«¯ä½¿ç”¨CPUè¿è¡Œæºå¤§æ¨¡å‹ã€‚
## 1. Yuan2.0-gguf

### 1.1 æ¦‚è¿°
æº2.0 æ˜¯æµªæ½®ä¿¡æ¯å‘å¸ƒçš„æ–°ä¸€ä»£åŸºç¡€è¯­è¨€å¤§æ¨¡å‹ã€‚æˆ‘ä»¬å¼€æºäº†å…¨éƒ¨çš„3ä¸ªæ¨¡å‹æº2.0-102Bï¼Œæº2.0-51Bå’Œæº2.0-2Bã€‚å¹¶ä¸”æˆ‘ä»¬æä¾›äº†é¢„è®­ç»ƒï¼Œå¾®è°ƒï¼Œæ¨ç†æœåŠ¡çš„ç›¸å…³è„šæœ¬ï¼Œä»¥ä¾›ç ”å‘äººå‘˜åšè¿›ä¸€æ­¥çš„å¼€å‘ã€‚æº2.0æ˜¯åœ¨æº1.0çš„åŸºç¡€ä¸Šï¼Œåˆ©ç”¨æ›´å¤šæ ·çš„é«˜è´¨é‡é¢„è®­ç»ƒæ•°æ®å’ŒæŒ‡ä»¤å¾®è°ƒæ•°æ®é›†ï¼Œä»¤æ¨¡å‹åœ¨è¯­ä¹‰ã€æ•°å­¦ã€æ¨ç†ã€ä»£ç ã€çŸ¥è¯†ç­‰ä¸åŒæ–¹é¢å…·å¤‡æ›´å¼ºçš„ç†è§£èƒ½åŠ›ã€‚

è¯¦æƒ…è¯·å‚è€ƒYuan2.0æ¨¡å‹
<a href="https://arxiv.org/ftp/arxiv/papers/2311/2311.15786.pdf" target="_blank">**æŠ€æœ¯æŠ¥å‘Š**</a>å’Œ
<a href="https://github.com/IEIT-Yuan/Yuan-2.0" target="_blank">**github**</a>


æœ¬é¡¹ç›®åŸºäºllama.cppï¼ˆversionï¼šb1742ï¼‰åœ¨Windowsç³»ç»Ÿï¼ˆCPU Onlyï¼‰ä¸Šå¯¹æº2.0-2Bæ¨¡å‹çš„é€‚é…ã€‚

ç”±äºæº2.0æ¨¡å‹ç»“æ„ä¸llamaç»“æ„å­˜åœ¨å·®å¼‚ï¼Œé’ˆå¯¹æº2.0æ¨¡å‹ï¼ˆYuan2.0-2Bï¼‰æ¨¡å‹LFAç»“æ„ï¼Œè¿›è¡Œå¦‚ä¸‹ä¿®æ”¹ï¼š

- llamaè®¡ç®—å›¾ä¸­æ·»åŠ LFAç»“æ„ï¼Œä¿®æ”¹ggml_conv_1dé€»è¾‘ï¼Œä»¥é€‚é…æº2.0ï¼Œä¿è¯å‰ååºåˆ—é•¿åº¦ä¸å˜ï¼›
- llamaè®¡ç®—å›¾ä¸­æ·»åŠ qã€kæ··åˆç›¸å…³çš„é€»è¾‘ï¼›
- ä¿®æ”¹IM2COLç®—å­ï¼Œä¿®æ”¹æ•°ç»„çš„è¯»å–æ–¹å¼ï¼›
- ä¿®æ”¹ADDç®—å­ï¼Œå°†å·ç§¯æ¨¡å—çš„è¾“å‡ºè¿›è¡Œè½¬ç½®ï¼Œä»¥é€‚é…åé¢çš„è®¡ç®—ï¼›
- ä¿®æ”¹concatç®—å­ï¼Œä»¥é€‚é…qã€kæ··åˆçš„é€»è¾‘ï¼›
- æ”¯æŒå¤šçº¿ç¨‹æ¨ç†ï¼ŒåŠ é€Ÿç”Ÿæˆé€Ÿç‡ï¼›

ç›®å‰æ”¯æŒfp16ç²¾åº¦æ¨¡å‹çš„ggufæ–‡ä»¶è½¬æ¢ï¼Œåç»­ä¼šæŒç»­è¿›è¡Œå…¶ä»–ç²¾åº¦çš„å·¥ä½œã€‚

### 1.2. ggufæ–‡ä»¶çš„ç”Ÿæˆ

```shell
python convert.py --model yuan2b-hf\yuan2-2B --outfile zh-models/Yuan2-2B-Februa-hf-GGUF.gguf
```

### 1.3. ç¼–è¯‘
  - Using `make`:
  - On Linux or MacOS:

      ```bash
      make
      ```

  - On Windows:

    1. Download the latest fortran version of [w64devkit](https://github.com/skeeto/w64devkit/releases).
    2. Extract `w64devkit` on your pc.
    3. Run `w64devkit.exe`.
    4. Use the `cd` command to reach the `llama.cpp` folder.
    5. From here you can run:
        ```bash
        make
        ```

- Using `CMake`:

    ```bash
    mkdir build
    cd build
    cmake ..
    cmake --build . --config Release


### 1.4. æµ‹è¯•Demo

### <font color=#FFC125 >1.4.1 æµ‹è¯•ç¯å¢ƒ </font>

- python3.9
- 11th Gen Intel(R) Core(TM) i5-1145G7 @2.60GHz 2.61GHz
- 8.00GB RAM
- Windows 10ä¸“ä¸šç‰ˆï¼ˆ21H1ï¼‰

### <font color=#FFC125 >1.4.2 æµ‹è¯•æ–¹æ³• </font>


```shell
main.exe -m D:\\llama-cpp\\llama.cpp\\zh-models\\Yuan2-2B-Februa-hf-GGUF.gguf -p "åŒ—äº¬ç®€ä»‹" -n 400  --top-k  5 --threads 4
```


### <font color=#FFC125 >1.4.3 æµ‹è¯•æ•ˆæœ </font>

```shell
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_new_context_with_model: KV self size  =   96.00 MiB, K (f16):   48.00 MiB, V (f16):   48.00 MiB
llama_build_graph: non-view tensors processed: 628/820
llama_build_graph: ****************************************************************
llama_build_graph: not all non-view tensors have been processed with a callback
llama_build_graph: this can indicate an inefficiency in the graph implementation
llama_build_graph: build with LLAMA_OFFLOAD_DEBUG for more info
llama_build_graph: ref: https://github.com/ggerganov/llama.cpp/pull/3837
llama_build_graph: ****************************************************************
llama_new_context_with_model: compute buffer total size = 270.94 MiB
Model metadata: {'tokenizer.ggml.add_eos_token': 'true', 'tokenizer.ggml.padding_token_id': '77185', 'tokenizer.ggml.seperator_token_id': '77185', 'tokenizer.ggml.eos_token_id': '77185', 'general.architecture': 'llama', 'llama.context_length': '8192', 'general.name': 'E:\\ckpts\\yuan2b-hf', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '2048', 'llama.feed_forward_length': '8192', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'llama.rope.dimension_count': '64', 'llama.attention.head_count': '32', 'tokenizer.ggml.bos_token_id': '77185', 'llama.block_count': '24', 'llama.attention.head_count_kv': '32', 'tokenizer.ggml.model': 'llama', 'general.file_type': '1'}
åŒ—äº¬æ˜¯ä¸­å›½çš„é¦–éƒ½ï¼Œæ˜¯ååŒ—åœ°åŒºçš„ä¸­å¿ƒåŸå¸‚ã€‚å®ƒæ˜¯ä¸­å›½æ”¿æ²»ã€æ–‡åŒ–å’Œç»æµçš„ä¸­å¿ƒï¼Œä¹Ÿæ˜¯ä¸­å›½æœ€å¤§çš„åŸå¸‚ä¹‹ä¸€ã€‚åŒ—äº¬å…·æœ‰æ‚ ä¹…çš„å†å²å’Œä¸°å¯Œçš„æ–‡åŒ–é—äº§ï¼ŒåŒ…æ‹¬é•¿åŸã€æ•…å®«ã€å¤©å›ç­‰ä¸–ç•Œé—»åçš„å¤è¿¹ï¼Œä»¥åŠç°ä»£åŒ–çš„æ‘©å¤©å¤§æ¥¼å’Œè´­ç‰©ä¸­å¿ƒã€‚
åŒ—äº¬ä¹Ÿæ˜¯ä¸­å›½çš„æ•™è‚²å’Œç§‘ç ”ä¸­å¿ƒï¼Œæ‹¥æœ‰ä¼—å¤šé«˜ç­‰é™¢æ ¡å’Œç§‘ç ”æœºæ„ï¼Œä¸ºä¸­å›½åŸ¹å…»äº†è®¸å¤šæ°å‡ºäººæ‰ã€‚æ­¤å¤–ï¼ŒåŒ—äº¬è¿˜æ˜¯ä¸­å›½é‡è¦çš„å•†ä¸šå’Œäº¤é€šä¸­å¿ƒï¼Œæ‹¥æœ‰ä¼—å¤šçš„å•†ä¸šåŒºå’Œè´­ç‰©ä¸­å¿ƒã€‚
åŒ—äº¬è¿˜æ˜¯ä¸€ä¸ªæ—…æ¸¸èƒœåœ°ï¼Œå¸å¼•ç€å¤§é‡çš„å›½å†…å¤–æ¸¸å®¢å‰æ¥å‚è§‚å’Œæ—…æ¸¸ã€‚è‘—åçš„æ™¯ç‚¹åŒ…æ‹¬æ•…å®«åšç‰©é™¢ã€é¢å’Œå›­ã€å¤©å›å…¬å›­ã€å…«è¾¾å²­é•¿åŸç­‰ã€‚è¿™äº›æ™¯ç‚¹éƒ½æœ‰ç€ç‹¬ç‰¹çš„å†å²æ–‡åŒ–èƒŒæ™¯å’Œå£®è§‚çš„è‡ªç„¶é£å…‰ï¼Œç»™äººä»¬å¸¦æ¥äº†ä¸åŒçš„ä½“éªŒã€‚
æ€»çš„æ¥è¯´ï¼ŒåŒ—äº¬æ˜¯ä¸€ä¸ªå†å²æ‚ ä¹…ã€æ–‡åŒ–åº•è•´ä¸°å¯Œã€æ—…æ¸¸èƒœåœ°ä¼—å¤šçš„åŸå¸‚ï¼Œå€¼å¾—ä¸€æ¸¸ã€‚æ— è®ºæ˜¯å¯¹äºè‰ºæœ¯çˆ±å¥½è€…ã€å†å²æ–‡åŒ–çˆ±å¥½è€…è¿˜æ˜¯è‡ªç„¶æ¢ç´¢è€…ï¼ŒåŒ—äº¬éƒ½æœ‰ç€ä¸å¯å¤šå¾—çš„ä½“éªŒã€‚


llama_print_timings:        load time =     579.04 ms
llama_print_timings:      sample time =     126.74 ms /   193 runs   (    0.66 ms per token,  1522.81 tokens per second)
llama_print_timings: prompt eval time =     578.98 ms /     4 tokens (  144.74 ms per token,     6.91 tokens per second)
llama_print_timings:        eval time =   20562.87 ms /   192 runs   (  107.10 ms per token,     9.34 tokens per second)
llama_print_timings:       total time =   22961.55 ms

```
### <font color=#FFC125 >1.4.4 æµ‹è¯•æ€§èƒ½ </font>


<table>
    <tr>
        <th rowspan="2">æ¨ç†æ€§èƒ½</th><th>GGUFæ ¼å¼ï¼ˆC++ï¼‰</th><th>HFæ ¼å¼ï¼ˆPythonï¼‰</th><th>åŠ é€Ÿæ¯”</th>
    </tr>
    <tr>
        <td align="center">9.16 tokens/s</td><td align="center">1.21 tokens/s</td><td align="center">7.57</td>
    </tr>
    <tr>
        <th rowspan="2">å†…å­˜å ç”¨</th><th>GGUFæ ¼å¼ï¼ˆC++ï¼‰</th><th>HFæ ¼å¼ï¼ˆPythonï¼‰</th><th>å†…å­˜å æ¯”ï¼ˆGGUF/HFï¼‰</th>
    </tr>
    <tr>
        <td align="center">~0.4 GB</td><td align="center">~8.6 GB</td><td align="center">4.65%</td>
    </tr>
</table>









## 2. Yuan2.0-M32-gguf

20240531æ›´æ–°

- æ”¯æŒyuan2bã€yuan2b-moeï¼›
- ä¿®æ”¹lfaè¾“å…¥ç¼“å­˜ä¸º2ä¸ªtokenï¼Œå†…å­˜å ç”¨å‡å°‘ï¼Œæ¨ç†é€Ÿåº¦ç•¥å¾®æå‡ï¼›
- ä¿®æ”¹-iäº¤äº’æ¨¡å¼ä¸‹çš„è‹¥å¹²bugï¼›

### 2.1 æ¦‚è¿°
æµªæ½®ä¿¡æ¯ **â€œæº2.0 M32â€å¤§æ¨¡å‹ï¼ˆç®€ç§°ï¼ŒYuan2.0-M32ï¼‰** é‡‡ç”¨ç¨€ç–æ··åˆä¸“å®¶æ¶æ„ï¼ˆMoEï¼‰ï¼Œä»¥Yuan2.0-2Bæ¨¡å‹ä½œä¸ºåŸºåº•æ¨¡å‹ï¼Œé€šè¿‡åˆ›æ–°çš„é—¨æ§ç½‘ç»œï¼ˆAttention Routerï¼‰å®ç°32ä¸ªä¸“å®¶é—´ï¼ˆExperts*32ï¼‰çš„ååŒå·¥ä½œä¸ä»»åŠ¡è°ƒåº¦ï¼Œåœ¨æ˜¾è‘—é™ä½æ¨¡å‹æ¨ç†ç®—åŠ›éœ€æ±‚çš„æƒ…å†µä¸‹ï¼Œå¸¦æ¥äº†æ›´å¼ºçš„æ¨¡å‹ç²¾åº¦è¡¨ç°ä¸æ¨ç†æ€§èƒ½ï¼›æº2.0-M32åœ¨å¤šä¸ªä¸šç•Œä¸»æµçš„è¯„æµ‹è¿›è¡Œäº†ä»£ç ç”Ÿæˆã€æ•°å­¦é—®é¢˜æ±‚è§£ã€ç§‘å­¦é—®ç­”ä¸ç»¼åˆçŸ¥è¯†èƒ½åŠ›ç­‰æ–¹é¢çš„èƒ½åŠ›æµ‹è¯„ã€‚ç»“æœæ˜¾ç¤ºï¼Œæº2.0-M32åœ¨å¤šé¡¹ä»»åŠ¡è¯„æµ‹ä¸­ï¼Œå±•ç¤ºå‡ºäº†è¾ƒä¸ºå…ˆè¿›çš„èƒ½åŠ›è¡¨ç°ï¼ŒMATHï¼ˆæ•°å­¦æ±‚è§£ï¼‰ã€ARC-Cï¼ˆç§‘å­¦é—®ç­”ï¼‰æµ‹è¯•ç²¾åº¦è¶…è¿‡LLaMA3-700äº¿æ¨¡å‹ã€‚**Yuan2.0-M32å¤§æ¨¡å‹** åŸºæœ¬ä¿¡æ¯å¦‚ä¸‹ï¼š

+ **æ¨¡å‹å‚æ•°é‡ï¼š** 40B <br>
+ **ä¸“å®¶æ•°é‡ï¼š** 32 <br>
+ **æ¿€æ´»ä¸“å®¶æ•°ï¼š** 2 <br>
+ **æ¿€æ´»å‚æ•°é‡ï¼š** 3.7B <br>
+ **è®­ç»ƒæ•°æ®é‡ï¼š** 2000B tokens <br>
+ **æ”¯æŒåºåˆ—é•¿åº¦ï¼š** 16K <br>

è¯¦æƒ…è¯·å‚è€ƒYuan2.0-M32æ¨¡å‹
<a href="https://arxiv.org/abs/2405.17976" target="_blank">**æŠ€æœ¯æŠ¥å‘Š**</a>å’Œ
<a href="https://github.com/IEIT-Yuan/Yuan2.0-M32" target="_blank">**github**</a>


### 2.2 å¦‚ä½•è½¬æ¢gguf
    # ç”±äºhfæ ¼å¼ckptçš„transformersç‰ˆæœ¬ä¸è½¬æ¢ggufçš„transformersä¸ä¸€è‡´ï¼Œéœ€è¦ä½¿ç”¨hfåŠ è½½ï¼Œç„¶åå†ä¿å­˜ä¸€ä¸‹ï¼›
    # å¯è¿è¡Œçš„ç¯å¢ƒå¦‚ä¸‹ï¼š
    python3.9 cpu win10/linux
    torch                     2.0.1                    pypi_0    pypi
    transformers              4.39.0                   pypi_0    pypi

    # transformersåŠ è½½
    model = AutoModelForCausalLM.from_pretrained(path, device_map="cpu", trust_remote_code=True,torch_dtype=torch.bfloat16).eval()
    # ä¿å­˜æ¨¡å‹
    torch.save(model.state_dict(),r"E:\ckpts\yuan2b-moe-2\pytorch_model_hf2-bk-fp16.bin")

    # ä½¿ç”¨æ–°ç”Ÿæˆçš„binï¼Œå†ä½¿ç”¨convert.pyï¼Œä¸ä¼šæŠ¥é”™

    convert.pyçš„ä¼ å…¥å‚æ•°model_typeï¼Œéœ€è¦æ ¹æ®æ¨¡å‹çš„ç»“æ„è¿›è¡Œä¿®æ”¹ï¼›

### 2.3 ç¼–è¯‘
    è·Ÿyuan2-ggufçš„ç¼–è¯‘æ–¹æ³•ä¸€æ ·ï¼›

### 2.4 æ¨ç†è„šæœ¬ï¼š
    windows:
    ./main.exe -m yuan2b-moe-40b-q4_0.gguf -t 6 -n 20 -p 'åŒ—äº¬ç®€ä»‹'

    linuxï¼š
    ./main -m /mnt/md0/yuan2b-moe-40b-24-f16-hf.gguf --file /mnt/md0/shenchong/llama.cpp-gitee/prompt_text.txt -c 1024 -b 1024 -t 64 -n 10


### 2.5 é‡åŒ–è„šæœ¬ï¼š
    windowsï¼š
    ./quantize.exe yuan2b-moe-40b-24-f16-hf.gguf yuan2b-moe-40b-q4_0.gguf q4_0

    linuxï¼š
    ./quantize yuan2b-moe-40b-24-f16-hf.gguf yuan2b-moe-40b-q4_0.gguf q4_0


## 3. FAQ

- 1ã€æ˜¯å¦æ”¯æŒgpuæ¨ç†ï¼Ÿæ˜¯å¦æ”¯æŒå…¶ä»–ggufæ¨¡å‹ï¼Ÿ
  - ç”±äºä¿®æ”¹äº†ggufåŸæœ‰çš„ä¸€äº›ç®—å­å®ç°ï¼Œè¯¥é¡¹ç›®ç›®å‰å¹¶ä¸èƒ½ä½¿ç”¨gpuè¿›è¡Œæ¨ç†ï¼Œè€Œä¸”ä¹Ÿä¸èƒ½è·‘llamaç­‰å…¶ä»–ggufæ¨¡å‹ï¼›

- 2ã€æ”¯æŒæ¨¡å‹åˆ—è¡¨
  - yuan2.0-2b
  - yuan2.0-m32



